{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfde2a0",
   "metadata": {},
   "source": [
    "# Lectura de datos avro\n",
    "Primero que todo, como nos interesa los datos de eventos transitorios, nos encontramos con una pagina web (https://ztf.uw.edu/alerts/public/) la cual es un telescopio que mapea cada día el cielo nocturno para obtener imagenes de eventos transitorios. Estos datos vienen en formato avro los cuales son archivos que almacenan datos en formato binario utilizando el esquema de los datos en un archivo contenedor.\n",
    "\n",
    "### Caracteristicas clave de los archivos avro\n",
    "- Serialización de datos:\n",
    "Avro serializa datos en un formato binario compacto y eficiente. \n",
    "- Esquemas definidos:\n",
    "Los datos están asociados con un esquema definido en formato JSON, lo que facilita la lectura y el procesamiento de los datos. \n",
    "- Evolución del esquema:\n",
    "Avro permite la evolución de los esquemas sin problemas, lo que significa que los programas antiguos pueden leer datos nuevos y viceversa. \n",
    "- Compatibilidad con diferentes lenguajes:\n",
    "Avro ofrece API para varias plataformas, incluyendo Java, Python, Ruby, C, C++ y más. \n",
    "- Integración con Hadoop:\n",
    "Avro es ampliamente utilizado en el ecosistema Hadoop para el almacenamiento y procesamiento de datos. \n",
    "- Ideal para streaming:\n",
    "Avro es adecuado para la transmisión de datos entre sistemas, ya que serializa los datos de manera independiente por filas. \n",
    "### Uso de archivos Avro:\n",
    "- Almacenamiento de datos:\n",
    "Avro es un formato de archivo útil para almacenar datos de forma persistente en sistemas de almacenamiento distribuido. \n",
    "- Transferencia de datos:\n",
    "Avro facilita la transferencia de datos entre sistemas y aplicaciones, especialmente en entornos de streaming. \n",
    "- Procesamiento de datos:\n",
    "Avro puede ser utilizado para el procesamiento de datos en paralelo, aprovechando las capacidades de los frameworks de computación distribuida como Apache Hadoop. \n",
    "- Integración con Kafka:\n",
    "Apache Kafka utiliza Avro para la serialización de mensajes, lo que permite la transmisión eficiente de datos. \n",
    "\n",
    "#### Información importante \n",
    "en nuestro caso nos benefició el formato avro ya que al compactarlos en formato binario, lo que nos entregan los archivo es un cubo 3d de imagenes 21x21 en el cual también se traen los metadatos, por lo tanto como nos interesa entrenar una red neuronal convolucional decidimos solamente extraer los datos de las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "from astropy.io import fits\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from astropy.io.fits.verify import VerifyWarning\n",
    "import os \n",
    "import glob\n",
    "\n",
    "def store_ztf_stamps_3d(input_dir, output_dir=\"ztf_3d_stamps\", crop_to_21x21=True):\n",
    "    \"\"\"\n",
    "    Procesa archivos Avro de ZTF en un directorio, extrae los stamps (science, reference, difference),\n",
    "    y los almacena en una matriz tridimensional (3, 21, 21) por alerta.\n",
    "\n",
    "    Parameteros:\n",
    "    - input_dir (str): Directorio con los archivos Avro.\n",
    "    - output_dir (str): Directorio donde se guardarán las matrices tridimensionales (.npy).\n",
    "    - crop_to_21x21 (bool): Si True, recorta los stamps a 21x21 píxeles (como en ALeRCE).\n",
    "    \"\"\"\n",
    "    # Crear directorio de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Ignorar advertencias de verificación de FITS\n",
    "    warnings.filterwarnings('ignore', category=VerifyWarning)\n",
    "\n",
    "    # Encontrar todos los archivos Avro en el directorio de entrada\n",
    "    avro_files = glob.glob(os.path.join(input_dir, \"*.avro\"))\n",
    "    if not avro_files:\n",
    "        print(f\"No se encontraron archivos Avro en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Procesando {len(avro_files)} archivos Avro en {input_dir}\")\n",
    "\n",
    "    # Procesar cada archivo Avro\n",
    "    for i, avro_file in enumerate(avro_files, 1):\n",
    "        print(f\"\\nLeyendo archivo {i}/{len(avro_files)}: {avro_file}\")\n",
    "        try:\n",
    "            with open(avro_file, 'rb') as f:\n",
    "                reader = fastavro.reader(f)\n",
    "                # Iterar sobre las alertas en el archivo\n",
    "                for alert in reader:\n",
    "                    # Extraer ID de la alerta\n",
    "                    object_id = alert.get('objectId', 'Unknown')\n",
    "                    print(f\"Procesando Alerta ID: {object_id}\")\n",
    "\n",
    "                    # Extraer imágenes (stamps)\n",
    "                    science_stamp = alert.get('cutoutScience', {}).get('stampData', None)\n",
    "                    reference_stamp = alert.get('cutoutTemplate', {}).get('stampData', None)\n",
    "                    difference_stamp = alert.get('cutoutDifference', {}).get('stampData', None)\n",
    "\n",
    "                    # Función para decodificar un stamp\n",
    "                    def decode_stamp(stamp_data):\n",
    "                        '''esta función se encarga de descomprimir el stamp, lo lee como un archivo FITS\n",
    "                        verifica tamaño para luego recotarlo en 21x21 reemplaza los NaN con 0 y devuelve\n",
    "                        el arreglo de imagen'''\n",
    "                        if stamp_data:\n",
    "                            try:\n",
    "                                # Descomprimir datos gzip\n",
    "                                decompressed_data = gzip.decompress(stamp_data)\n",
    "                                # Leer datos FITS desde BytesIO\n",
    "                                with fits.open(BytesIO(decompressed_data), ignore_missing_simple=True) as hdu_list:\n",
    "                                    stamp_array = hdu_list[0].data.astype(np.float32)\n",
    "                                # Verificar tamaño esperado (63x63 para ZTF)\n",
    "                                if stamp_array.shape != (63, 63):\n",
    "                                    print(f\"Advertencia: El stamp tiene tamaño {stamp_array.shape}, esperado (63, 63)\")\n",
    "                                    return None\n",
    "                                # Recortar a 21x21 píxeles si se solicita\n",
    "                                if crop_to_21x21:\n",
    "                                    stamp_array = stamp_array[21:42, 21:42]\n",
    "                                # Reemplazar NaN por 0\n",
    "                                stamp_array = np.nan_to_num(stamp_array, nan=0.0)\n",
    "                                return stamp_array\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error al decodificar stamp: {e}\")\n",
    "                                return None\n",
    "                        return None\n",
    "\n",
    "                    # Decodificar los stamps\n",
    "                    science_array = decode_stamp(science_stamp)\n",
    "                    reference_array = decode_stamp(reference_stamp)\n",
    "                    difference_array = decode_stamp(difference_stamp)\n",
    "\n",
    "                    # Verificar que todos los stamps se cargaron correctamente\n",
    "                    if science_array is not None and reference_array is not None and difference_array is not None:\n",
    "                        # Crear matriz tridimensional (3, 21, 21) o (3, 63, 63) según crop_to_21x21\n",
    "                        stamp_size = 21 if crop_to_21x21 else 63\n",
    "                        stamps_3d = np.zeros((3, stamp_size, stamp_size), dtype=np.float32)\n",
    "                        stamps_3d[0] = science_array\n",
    "                        stamps_3d[1] = reference_array\n",
    "                        stamps_3d[2] = difference_array\n",
    "\n",
    "                        # Guardar la matriz tridimensional como archivo .npy\n",
    "                        output_path = os.path.join(output_dir, f'alert_{object_id}_stamps_3d.npy')\n",
    "                        np.save(output_path, stamps_3d)\n",
    "                        print(f\"Matriz 3D guardada en: {output_path} (forma: {stamps_3d.shape})\")\n",
    "                    else:\n",
    "                        print(f\"No se pudieron cargar todos los stamps para la alerta {object_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo {avro_file}: {e}\")\n",
    "# Ejemplo de uso\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/seba/Usach/bigdata/Proyecto/datos4\"  # Reemplaza con la ruta al directorio con archivos Avro\n",
    "    store_ztf_stamps_3d(input_directory, output_dir=\"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_4\", crop_to_21x21=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c113d56",
   "metadata": {},
   "source": [
    "# datos etiquetados \n",
    "Como nos interesa cerar una CNN para clasificar eventos transitorios necesitamos de datos etiquetados, los cuales pudimos encontrar en  https://zenodo.org/records/4279623 para luego realizar un crossmatch con nuestros datos a partir del id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd17c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alerce_data = pd.read_csv(\"/home/seba/Usach/bigdata/Proyecto/etiquetas/ALeRCE_lc_classifier_outputs_ZTF_unlabeled_set_20200609.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb6bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Etiquetas en alerce_data['predicted_class']:\n",
      "predicted_class\n",
      "Periodic-Other    243374\n",
      "E                 198122\n",
      "LPV               161592\n",
      "YSO                85087\n",
      "RRL                58592\n",
      "QSO                43054\n",
      "DSCT               26672\n",
      "CEP                17307\n",
      "AGN                14342\n",
      "CV/Nova             7945\n",
      "Blazar              5085\n",
      "SNIa                3956\n",
      "SNIbc               1626\n",
      "SNII                 890\n",
      "SLSN                 727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Muestra las etiquetas únicas y su número total en alerce_data\n",
    "print(\"\\nEtiquetas en alerce_data['predicted_class']:\")\n",
    "print(alerce_data['predicted_class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee43134",
   "metadata": {},
   "source": [
    "#### Data sets\n",
    "Nos interesa obtener grandes volumenes de datos para el entrenamiento, por lo tanto realizamos un codigo que se encarga de sacar variaos set de datos y juntarlos en un puro dataframe, vale recalcar que los datos son los extraidos anteriormente en binario (formato .npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844117af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358163, 2)ocesados en carpeta 4: 147156 de 147156\n",
      "      object_id                                               data\n",
      "0  ZTF25aapoqyu  [[[291.5396, 300.09894, 299.94568, 299.8415, 3...\n",
      "1  ZTF21aayefhf  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
      "2  ZTF21acnhdfa  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 250...\n",
      "3  ZTF19acsjkbr  [[[234.52643, 238.25146, 243.91174, 239.44464,...\n",
      "4  ZTF21aamjywk  [[[286.34802, 323.42926, 302.9098, 286.79755, ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folders = [\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_2\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_3\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_4\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, folder in enumerate(folders, 1):\n",
    "    file_list = glob.glob(os.path.join(folder, \"alert_*_stamps_3d.npy\"))\n",
    "    total_files = len(file_list)\n",
    "    for j, filepath in enumerate(file_list, 1):\n",
    "        filename = os.path.basename(filepath)\n",
    "        object_id = filename.split('_')[1]\n",
    "        data = np.load(filepath)\n",
    "        rows.append({'object_id': object_id, 'data': data})\n",
    "        print(f'Archivos procesados en carpeta {i}: {j} de {total_files}', end='\\r')\n",
    "\n",
    "\n",
    "avro_data = pd.DataFrame(rows) # crear un dataframe con columnas: object_id, data\n",
    "avro_data = avro_data.drop_duplicates(subset='object_id', keep='first') # eliminamos duplicados con el mismo object_id\n",
    "\n",
    "print(avro_data.shape)\n",
    "print(avro_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fb618",
   "metadata": {},
   "source": [
    "# Cross Match\n",
    "ahora que tenemos el set de datos toca realizar el crossmatch, esto es importante ya que la red neuronal debe de saber que clase de evento transitorio está analizando (en nuestro caso nos decantamos por QSO, YSO y AGN, también añadimos \"otros\"), ademas de que el set de datos etiquetados no tiene imagenes y por lo tanto es completamente necesario realizar el crossmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Otro    122968\n",
      "YSO       6935\n",
      "QSO       2804\n",
      "AGN       1359\n",
      "Name: count, dtype: int64\n",
      "      object_id                                               data label\n",
      "0  ZTF18acvwapg  [[[284.2307, 270.86517, 263.56464, 281.25702, ...  Otro\n",
      "1  ZTF18adalcju  [[[366.73608, 359.24762, 350.893, 348.76733, 3...  Otro\n",
      "2  ZTF19aahibom  [[[337.14615, 369.51688, 363.7308, 388.83627, ...  Otro\n",
      "3  ZTF18adbzbnk  [[[295.16556, 283.30942, 293.73746, 276.49637,...  Otro\n",
      "4  ZTF19aaskrrm  [[[349.99973, 350.7228, 340.42014, 354.07413, ...  Otro\n"
     ]
    }
   ],
   "source": [
    "# Crear una nueva columna de etiqueta, asignando 'Otro' a los que no son QSO, YSO o AGN\n",
    "alerce_data['label'] = alerce_data['predicted_class'].where(\n",
    "    alerce_data['predicted_class'].isin(['QSO', 'YSO', 'AGN']),\n",
    "    other='Otro'\n",
    ")\n",
    "\n",
    "# Seleccionar solo las columnas necesarias y renombrar para el merge\n",
    "alerce_labels = alerce_data[['oid', 'label']].rename(columns={'oid': 'object_id'})\n",
    "\n",
    "# Hacer el cross-match con avro_data\n",
    "final_avro_df = pd.merge(avro_data, alerce_labels, on='object_id', how='inner')\n",
    "\n",
    "print(final_avro_df['label'].value_counts())\n",
    "print(final_avro_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7416a2",
   "metadata": {},
   "source": [
    "#### Guardar Dataset en varios csv\n",
    "en nuestro caso nos acomoda guardar los datos en csv, debido a que sabemos manejar este tipo de datos  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75985f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtramos por cada clase y tomamos una muestra eleatoria de 12000 ejemplos de la clase 'otro' para balancear el dataset\n",
    "# finalmente como nos indico nuestro especialista en redes neuronales, guardaremos cada clase en un csv separado\n",
    "Otro = final_avro_df[final_avro_df['label'] == 'Otro']\n",
    "Otro = Otro.sample(n=12000)\n",
    "\n",
    "Quasar = final_avro_df[final_avro_df['label'] == 'QSO']\n",
    "AGN = final_avro_df[final_avro_df['label'] == 'AGN']\n",
    "YSO = final_avro_df[final_avro_df['label'] == 'YSO']\n",
    "\n",
    "\n",
    "Quasar.to_csv(\"ztf_avro_Grande_QSO.csv\", index=False)\n",
    "AGN.to_csv(\"ztf_avro_Grande_AGN.csv\", index=False)\n",
    "YSO.to_csv(\"ztf_avro_Grande_YSO.csv\", index=False)\n",
    "Otro.to_csv(\"ztf_avro_Grande_Otro.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16585244",
   "metadata": {},
   "source": [
    "# En resumen\n",
    "### el codigo lo que hace es:\n",
    "- procesar los datos de alerta ZTF desde archivos avro\n",
    "- extraer las imagenes en formato 3D (3 canales) para cada alerta (también se limpian los datos transformando los NaN a 0)\n",
    "- unimos los datos con etiquetas entregadas por alerce (hacemos un crossmatch con un dataset pequeño el cual tiene datos etiquetados)\n",
    "- filtramos por clases de nuestro interes y agregamos una nueva clase para evitar error de clasificación (QSO,YSO,AGN,Otro)\n",
    "- exportamos el conjunto de datos etiquetados en CSV para futuros entrenamientos de la CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7ec3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
