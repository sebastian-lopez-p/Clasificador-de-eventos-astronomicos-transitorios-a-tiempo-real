{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfde2a0",
   "metadata": {},
   "source": [
    "# Lectura de datos avro\n",
    "Primero que todo, como nos interesa los datos de eventos transitorios, nos encontramos con una pagina web (https://ztf.uw.edu/alerts/public/) la cual es un telescopio que mapea cada día el cielo nocturno para obtener imagenes de eventos transitorios. Estos datos vienen en formato avro los cuales son archivos que almacenan datos en formato binario utilizando el esquema de los datos en un archivo contenedor.\n",
    "\n",
    "### Caracteristicas clave de los archivos avro\n",
    "- Serialización de datos:\n",
    "Avro serializa datos en un formato binario compacto y eficiente. \n",
    "- Esquemas definidos:\n",
    "Los datos están asociados con un esquema definido en formato JSON, lo que facilita la lectura y el procesamiento de los datos. \n",
    "- Evolución del esquema:\n",
    "Avro permite la evolución de los esquemas sin problemas, lo que significa que los programas antiguos pueden leer datos nuevos y viceversa. \n",
    "- Compatibilidad con diferentes lenguajes:\n",
    "Avro ofrece API para varias plataformas, incluyendo Java, Python, Ruby, C, C++ y más. \n",
    "- Integración con Hadoop:\n",
    "Avro es ampliamente utilizado en el ecosistema Hadoop para el almacenamiento y procesamiento de datos. \n",
    "- Ideal para streaming:\n",
    "Avro es adecuado para la transmisión de datos entre sistemas, ya que serializa los datos de manera independiente por filas. \n",
    "### Uso de archivos Avro:\n",
    "- Almacenamiento de datos:\n",
    "Avro es un formato de archivo útil para almacenar datos de forma persistente en sistemas de almacenamiento distribuido. \n",
    "- Transferencia de datos:\n",
    "Avro facilita la transferencia de datos entre sistemas y aplicaciones, especialmente en entornos de streaming. \n",
    "- Procesamiento de datos:\n",
    "Avro puede ser utilizado para el procesamiento de datos en paralelo, aprovechando las capacidades de los frameworks de computación distribuida como Apache Hadoop. \n",
    "- Integración con Kafka:\n",
    "Apache Kafka utiliza Avro para la serialización de mensajes, lo que permite la transmisión eficiente de datos. \n",
    "\n",
    "#### Información importante \n",
    "En nuestro caso nos benefició el formato avro ya que al compactarlos en formato binario, lo que nos entregan los archivo es un cubo 3d de imágenes 21x21 en el cual también se traen los metadatos, por lo tanto como nos interesa entrenar una red neuronal convolucional decidimos solamente extraer los datos de las imágenes y metadatos utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "from astropy.io import fits\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from astropy.io.fits.verify import VerifyWarning\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_ztf_stamps_3d_with_metadata(input_dir, output_dir=\"ztf_3d_stamps\", crop_to_21x21=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    avro_files = glob.glob(os.path.join(input_dir, \"*.avro\"))\n",
    "    if not avro_files:\n",
    "        print(f\"No se encontraron archivos Avro en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Procesando {len(avro_files)} archivos Avro en {input_dir}\")\n",
    "\n",
    "    for i, avro_file in enumerate(avro_files, 1):\n",
    "\n",
    "        if i % 500 == 0: \n",
    "            print(f\"Leyendo archivo {i}/{len(avro_files)}: {avro_file}     \", end='\\r')\n",
    "        try:\n",
    "            with open(avro_file, 'rb') as f:\n",
    "                reader = fastavro.reader(f)\n",
    "                for alert in reader:\n",
    "                    object_id = alert.get('objectId', 'Unknown')\n",
    "                    candidate = alert.get('candidate', {})\n",
    "\n",
    "                    science_stamp = alert.get('cutoutScience', {}).get('stampData', None)\n",
    "                    reference_stamp = alert.get('cutoutTemplate', {}).get('stampData', None)\n",
    "                    difference_stamp = alert.get('cutoutDifference', {}).get('stampData', None)\n",
    "\n",
    "                    def decode_stamp(stamp_data):\n",
    "                        if stamp_data:\n",
    "                            try:\n",
    "                                decompressed = gzip.decompress(stamp_data)\n",
    "                                with fits.open(BytesIO(decompressed), ignore_missing_simple=True) as hdu:\n",
    "                                    arr = hdu[0].data.astype(np.float32)\n",
    "                                if arr.shape != (63, 63):\n",
    "                                    \n",
    "                                    return None\n",
    "                                if crop_to_21x21:\n",
    "                                    arr = arr[21:42, 21:42]\n",
    "                                return np.nan_to_num(arr, nan=0.0)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error decodificando stamp: {e}\")\n",
    "                        return None\n",
    "\n",
    "                    sci = decode_stamp(science_stamp)\n",
    "                    ref = decode_stamp(reference_stamp)\n",
    "                    diff = decode_stamp(difference_stamp)\n",
    "\n",
    "                    if sci is not None and ref is not None and diff is not None:\n",
    "                        stamps_3d = np.stack([sci, ref, diff])\n",
    "\n",
    "                        # Recolectar metadatos relevantes\n",
    "                        metadata = {\n",
    "                            \"ra\": candidate.get(\"ra\", -999),\n",
    "                            \"dec\": candidate.get(\"dec\", -999),\n",
    "                            \"magpsf\": candidate.get(\"magpsf\", -999),\n",
    "                            \"sigmapsf\": candidate.get(\"sigmapsf\", -999),\n",
    "                            \"isdiffpos\": int(candidate.get(\"isdiffpos\", 'f') == 't'),\n",
    "                            \"diffmaglim\": candidate.get(\"diffmaglim\", -999),\n",
    "                            \"fwhm\": candidate.get(\"fwhm\", -999),\n",
    "                            \"sgscore1\": candidate.get(\"sgscore1\", -999),\n",
    "                            \"sgscore2\": candidate.get(\"sgscore2\", -999),\n",
    "                            \"sgscore3\": candidate.get(\"sgscore3\", -999),\n",
    "                            \"distpsnr1\": candidate.get(\"distpsnr1\", -999),\n",
    "                            \"distpsnr2\": candidate.get(\"distpsnr2\", -999),\n",
    "                            \"distpsnr3\": candidate.get(\"distpsnr3\", -999),\n",
    "                            \"classtar\": candidate.get(\"classtar\", -999),\n",
    "                            \"ndethist\": candidate.get(\"ndethist\", -1),\n",
    "                            \"ncovhist\": candidate.get(\"ncovhist\", -1),\n",
    "                            \"chinr\": candidate.get(\"chinr\", -999),\n",
    "                            \"sharpnr\": candidate.get(\"sharpnr\", -999),\n",
    "                            \"gal_lat\": alert.get(\"gal_lat\", -999),\n",
    "                            \"gal_lng\": alert.get(\"gal_lng\", -999),\n",
    "                            \"ecl_lat\": alert.get(\"ecl_lat\", -999),\n",
    "                            \"ecl_lng\": alert.get(\"ecl_lng\", -999),\n",
    "                            \"approx_nondet\": candidate.get(\"ncovhist\", -1) - candidate.get(\"ndethist\", -1),\n",
    "                        }\n",
    "\n",
    "                        # Guardar como dict\n",
    "                        data_dict = {\n",
    "                            \"object_id\": object_id,\n",
    "                            \"data\": stamps_3d,\n",
    "                            \"metadata\": metadata\n",
    "                        }\n",
    "\n",
    "                        np.save(os.path.join(output_dir, f\"alert_{object_id}_full.npy\"), data_dict)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en archivo {avro_file}: {e}\")\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Normaliza los datos de entrada a un rango de 0 a 1.\n",
    "    \"\"\"\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    return (data - data_min) / (data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86167b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/seba/Usach/bigdata/Proyecto/datos1\"  # Reemplaza con la ruta al directorio con archivos Avro\n",
    "    store_ztf_stamps_3d_with_metadata(input_directory, output_dir=\"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_1\", crop_to_21x21=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c113d56",
   "metadata": {},
   "source": [
    "# Datos etiquetados \n",
    "Como nos interesa crear una CNN para clasificar eventos transitorios necesitamos de datos etiquetados, los cuales pudimos encontrar en  https://zenodo.org/records/4279623 para luego realizar un crossmatch con nuestros datos a partir del id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd17c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alerce_data = pd.read_csv(\"/home/seba/Usach/bigdata/Proyecto/etiquetas/ALeRCE_lc_classifier_outputs_ZTF_unlabeled_set_20200609.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb6bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Etiquetas en alerce_data['predicted_class']:\n",
      "predicted_class\n",
      "Periodic-Other    243374\n",
      "E                 198122\n",
      "LPV               161592\n",
      "YSO                85087\n",
      "RRL                58592\n",
      "QSO                43054\n",
      "DSCT               26672\n",
      "CEP                17307\n",
      "AGN                14342\n",
      "CV/Nova             7945\n",
      "Blazar              5085\n",
      "SNIa                3956\n",
      "SNIbc               1626\n",
      "SNII                 890\n",
      "SLSN                 727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Muestra las etiquetas únicas y su número total en alerce_data\n",
    "print(\"\\nEtiquetas en alerce_data['predicted_class']:\")\n",
    "print(alerce_data['predicted_class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee43134",
   "metadata": {},
   "source": [
    "#### Data sets\n",
    "Nos interesa obtener grandes volumenes de datos para el entrenamiento, por lo tanto realizamos un codigo que se encarga de sacar variaos set de datos y juntarlos en un puro dataframe, vale recalcar que los datos son los extraidos anteriormente en binario (formato .npy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844117af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358163, 2)ocesados en carpeta 4: 147156 de 147156\n",
      "      object_id                                               data\n",
      "0  ZTF25aapoqyu  [[[291.5396, 300.09894, 299.94568, 299.8415, 3...\n",
      "1  ZTF21aayefhf  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
      "2  ZTF21acnhdfa  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 250...\n",
      "3  ZTF19acsjkbr  [[[234.52643, 238.25146, 243.91174, 239.44464,...\n",
      "4  ZTF21aamjywk  [[[286.34802, 323.42926, 302.9098, 286.79755, ...\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_2\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_3\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_4\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for folder in folders:\n",
    "    file_list = glob.glob(os.path.join(folder, \"alert_*_full.npy\"))\n",
    "    total_files = len(file_list)\n",
    "\n",
    "    for j, path in enumerate(file_list, 1):\n",
    "        try:\n",
    "            data = np.load(path, allow_pickle=True).item()\n",
    "            rows.append({\n",
    "                \"object_id\": data[\"object_id\"],\n",
    "                \"data\": data[\"data\"],\n",
    "                **data[\"metadata\"]  # Desempaquetamos el dict de metadatos como columnas\n",
    "            })\n",
    "            print(f'Archivos procesados: {j} de {total_files}  ', end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando {path}: {e}\")\n",
    "\n",
    "\n",
    "# Convertir a DataFrame\n",
    "avro_df = pd.DataFrame(rows)\n",
    "avro_df = avro_df.drop_duplicates(subset='object_id', keep='first') # eliminamos duplicados con el mismo object_id\n",
    "\n",
    "print(avro_df.shape)\n",
    "print(avro_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fb618",
   "metadata": {},
   "source": [
    "# Cross Match\n",
    "Ahora que tenemos el set de datos toca realizar el crossmatch, esto es importante ya que la red neuronal debe de saber que clase de evento transitorio está analizando (en nuestro caso nos decantamos por QSO, YSO y AGN, también añadimos \"otros\"), ademas de que el set de datos etiquetados no tiene imagenes y por lo tanto es completamente necesario realizar el crossmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Otro    122968\n",
      "YSO       6935\n",
      "QSO       2804\n",
      "AGN       1359\n",
      "Name: count, dtype: int64\n",
      "      object_id                                               data label\n",
      "0  ZTF18acvwapg  [[[284.2307, 270.86517, 263.56464, 281.25702, ...  Otro\n",
      "1  ZTF18adalcju  [[[366.73608, 359.24762, 350.893, 348.76733, 3...  Otro\n",
      "2  ZTF19aahibom  [[[337.14615, 369.51688, 363.7308, 388.83627, ...  Otro\n",
      "3  ZTF18adbzbnk  [[[295.16556, 283.30942, 293.73746, 276.49637,...  Otro\n",
      "4  ZTF19aaskrrm  [[[349.99973, 350.7228, 340.42014, 354.07413, ...  Otro\n"
     ]
    }
   ],
   "source": [
    "# Definir el mapeo de clases a las nuevas etiquetas\n",
    "label_map = {\n",
    "    'AGN': 'AGN',\n",
    "    'Blazar': 'AGN',\n",
    "    'QSO': 'QSO',\n",
    "    'YSO': 'YSO',\n",
    "    'E': 'VS',\n",
    "    'LPV': 'VS',\n",
    "    'RRL': 'VS',\n",
    "    'DSCT': 'VS',\n",
    "    'CEP': 'VS',\n",
    "    'SNIa': 'SN',\n",
    "    'SNIbc': 'SN',\n",
    "    'SNII': 'SN',\n",
    "    'SLSN': 'SN',\n",
    "    'Periodic-Other': 'Other',\n",
    "    'CV/Nova': 'Other'\n",
    "}\n",
    "\n",
    "# Asignar la nueva etiqueta según el mapeo, si no está en el mapeo se pone 'Other'\n",
    "alerce_data['label'] = alerce_data['predicted_class'].map(label_map).fillna('Other')\n",
    "\n",
    "# Seleccionar solo las columnas necesarias y renombrar para el merge\n",
    "alerce_labels = alerce_data[['oid', 'label']].rename(columns={'oid': 'object_id'})\n",
    "\n",
    "# Hacer el cross-match con avro_data\n",
    "final_avro_df = pd.merge(avro_df, alerce_labels, on='object_id', how='inner')\n",
    "\n",
    "print(final_avro_df['label'].value_counts())\n",
    "print(final_avro_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7416a2",
   "metadata": {},
   "source": [
    "#### Guardar Dataset en varios csv\n",
    "En nuestro caso nos acomoda guardar los datos en pickle, debido a que estamos trabajando con matrices de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75985f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos por cada clase y tomamos una muestra eleatoria de 12000 ejemplos de la clase 'Other' y 'Variable_Star'para balancear el dataset\n",
    "\n",
    "Other = final_avro_df[final_avro_df['label'] == 'Other'].sample(n=12000, random_state=42)\n",
    "Other = Other.reset_index(drop=True)\n",
    "Other['data'] = Other['data'].apply(normalize_data)\n",
    "\n",
    "Quasar = final_avro_df[final_avro_df['label'] == 'QSO']\n",
    "Quasar = Quasar.reset_index(drop=True)\n",
    "Quasar['data'] = Quasar['data'].apply(normalize_data)\n",
    "\n",
    "AGN = final_avro_df[final_avro_df['label'] == 'AGN']\n",
    "AGN = AGN.reset_index(drop=True)\n",
    "AGN['data'] = AGN['data'].apply(normalize_data)\n",
    "\n",
    "YSO = final_avro_df[final_avro_df['label'] == 'YSO']\n",
    "YSO = YSO.reset_index(drop=True)\n",
    "YSO['data'] = YSO['data'].apply(normalize_data)\n",
    "\n",
    "Variable_Star = final_avro_df[final_avro_df['label'] == 'VS'].sample(n=12000, random_state=42)\n",
    "Variable_Star = Variable_Star.reset_index(drop=True)\n",
    "Variable_Star['data'] = Variable_Star['data'].apply(normalize_data)\n",
    "\n",
    "Super_nova = final_avro_df[final_avro_df['label'] == 'SN']\n",
    "Super_nova = Super_nova.reset_index(drop=True)\n",
    "Super_nova['data'] = Super_nova['data'].apply(normalize_data)\n",
    "\n",
    "\n",
    "# Finalmente como nos indico nuestro especialista en redes neuronales, \n",
    "# guardaremos cada clase en un .pickle separado\n",
    "Quasar.to_pickle(\"ztf_avro_Grande_QSO.pkl\")\n",
    "AGN.to_pickle(\"ztf_avro_Grande_AGN.pkl\")\n",
    "YSO.to_pickle(\"ztf_avro_Grande_YSO.pkl\")\n",
    "Other.to_pickle(\"ztf_avro_Grande_Other.pkl\")\n",
    "Variable_Star.to_pickle(\"ztf_avro_Grande_Variable_Star.pkl\")\n",
    "Super_nova.to_pickle(\"ztf_avro_Grande_Super_nova.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16585244",
   "metadata": {},
   "source": [
    "# En resumen\n",
    "### El codigo lo que hace es:\n",
    "- Procesar los datos de alerta ZTF desde archivos avro.\n",
    "- Extraer las imagenes en formato 3D (3 canales) para cada alerta (también se limpian los datos transformando los NaN a 0).\n",
    "- Unimos los datos con etiquetas entregadas por alerce (hacemos un crossmatch con un dataset pequeño el cual tiene datos etiquetados).\n",
    "- Filtramos por clases de nuestro interes y agregamos una nueva clase para evitar error de clasificación (QSO,YSO,AGN,VS,SN,Otro).\n",
    "- Exportamos el conjunto de datos etiquetados en pickle para futuros entrenamientos de la CNN ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
