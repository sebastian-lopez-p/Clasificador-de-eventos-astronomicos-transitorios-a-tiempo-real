{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfde2a0",
   "metadata": {},
   "source": [
    "# Lectura de datos avro\n",
    "Primero que todo, como nos interesa los datos de eventos transitorios, nos encontramos con una pagina web (https://ztf.uw.edu/alerts/public/) la cual es un telescopio que mapea cada día el cielo nocturno para obtener imagenes de eventos transitorios. Estos datos vienen en formato avro los cuales son archivos que almacenan datos en formato binario utilizando el esquema de los datos en un archivo contenedor.\n",
    "\n",
    "### Caracteristicas clave de los archivos avro\n",
    "- Serialización de datos:\n",
    "Avro serializa datos en un formato binario compacto y eficiente. \n",
    "- Esquemas definidos:\n",
    "Los datos están asociados con un esquema definido en formato JSON, lo que facilita la lectura y el procesamiento de los datos. \n",
    "- Evolución del esquema:\n",
    "Avro permite la evolución de los esquemas sin problemas, lo que significa que los programas antiguos pueden leer datos nuevos y viceversa. \n",
    "- Compatibilidad con diferentes lenguajes:\n",
    "Avro ofrece API para varias plataformas, incluyendo Java, Python, Ruby, C, C++ y más. \n",
    "- Integración con Hadoop:\n",
    "Avro es ampliamente utilizado en el ecosistema Hadoop para el almacenamiento y procesamiento de datos. \n",
    "- Ideal para streaming:\n",
    "Avro es adecuado para la transmisión de datos entre sistemas, ya que serializa los datos de manera independiente por filas. \n",
    "### Uso de archivos Avro:\n",
    "- Almacenamiento de datos:\n",
    "Avro es un formato de archivo útil para almacenar datos de forma persistente en sistemas de almacenamiento distribuido. \n",
    "- Transferencia de datos:\n",
    "Avro facilita la transferencia de datos entre sistemas y aplicaciones, especialmente en entornos de streaming. \n",
    "- Procesamiento de datos:\n",
    "Avro puede ser utilizado para el procesamiento de datos en paralelo, aprovechando las capacidades de los frameworks de computación distribuida como Apache Hadoop. \n",
    "- Integración con Kafka:\n",
    "Apache Kafka utiliza Avro para la serialización de mensajes, lo que permite la transmisión eficiente de datos. \n",
    "\n",
    "#### Información importante \n",
    "En nuestro caso nos benefició el formato avro ya que al compactarlos en formato binario, lo que nos entregan los archivo es un cubo 3d de imagenes 21x21 en el cual también se traen los metadatos, por lo tanto como nos interesa entrenar una red neuronal con capas convolucionales y densas, decidimos  extraer los datos de las imagenes (para la capa conv) y el metadata (para la capa densa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4233896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 2549 archivos Avro en /home/seba/Usach/bigdata/Proyecto/datos1\n",
      "Leyendo archivo 2500/2549: /home/seba/Usach/bigdata/Proyecto/datos1/3079428843515015018.avro     \r"
     ]
    }
   ],
   "source": [
    "\n",
    "def store_ztf_stamps_3d_with_metadata(input_dir, output_dir=\"ztf_3d_stamps\", crop_to_21x21=True):\n",
    "    \"\"\" store_ztf_stamps_3d_with_metadata se encarga de leer archivos AVRO de alertas\n",
    "     astronomicas del ZTF, extrae las imagenes (las de ciencia, referencia y diferencia)\n",
    "     las convierte en imagenes 2D de 21x21 pixeles, los apila en un arreglo 3D y guarda \n",
    "     todo al final como un archivo .npy junto a los metadatos\n",
    "     \n",
    "     input:\n",
    "     input_dir: corresponde al directorio donde se encuentran los archivos.avro\n",
    "     \n",
    "     output:\n",
    "     un archivo llamado alert_{object_id}_full.npy donde el objet_id es el id del evento \n",
    "     transitorio (los cuales seran guardados en el directorio ztf_3d_stamps)\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Nos aseguramos de que haya un archivo.avro\n",
    "    avro_files = glob.glob(os.path.join(input_dir, \"*.avro\"))\n",
    "    if not avro_files:\n",
    "        print(f\"No se encontraron archivos Avro en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Procesando {len(avro_files)} archivos Avro en {input_dir}\")\n",
    "    # Recorre los archivos avro de la carpeta\n",
    "    for i, avro_file in enumerate(avro_files, 1):\n",
    "\n",
    "        if i % 500 == 0: \n",
    "            print(f\"Leyendo archivo {i}/{len(avro_files)}: {avro_file}     \", end='\\r')\n",
    "        try:\n",
    "            with open(avro_file, 'rb') as f:\n",
    "                reader = fastavro.reader(f)\n",
    "                for alert in reader:\n",
    "                    object_id = alert.get('objectId', 'Unknown')\n",
    "                    candidate = alert.get('candidate', {})\n",
    "\n",
    "                    science_stamp = alert.get('cutoutScience', {}).get('stampData', None)\n",
    "                    reference_stamp = alert.get('cutoutTemplate', {}).get('stampData', None)\n",
    "                    difference_stamp = alert.get('cutoutDifference', {}).get('stampData', None)\n",
    "\n",
    "                    def decode_stamp(stamp_data): \n",
    "                        \"\"\" decode_stamp verifica que haya datos con un if, descomprime los datos \n",
    "                        los cuales vienen en formato gzip, abre el contenido FITS con astropy\n",
    "                        convierte los datos de los FITS en un arreglo NumPy si que es que el arreglo no tiene \n",
    "                        21x21 pixeles los recorta, reemplaza los nan por cero y finalmente devuelve el valor\n",
    "                        limpio\n",
    "\n",
    "                        input: \n",
    "                        stamp_data: los datos de las imagenes\n",
    "\n",
    "                        output: \n",
    "                        retorna una imagen 2D de tamaño (63,63) o (21,21).\n",
    "                        si hay error o el tamaño no es el esperado: None \"\"\"\n",
    "                        \n",
    "                        if stamp_data:\n",
    "                            try:\n",
    "                                decompressed = gzip.decompress(stamp_data)\n",
    "                                with fits.open(BytesIO(decompressed), ignore_missing_simple=True) as hdu:\n",
    "                                    arr = hdu[0].data.astype(np.float32)\n",
    "                                if arr.shape != (63, 63):\n",
    "                                    \n",
    "                                    return None\n",
    "                                if crop_to_21x21:\n",
    "                                    arr = arr[21:42, 21:42]\n",
    "                                return np.nan_to_num(arr, nan=0.0)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error decodificando stamp: {e}\")\n",
    "                        return None\n",
    "\n",
    "                    sci = decode_stamp(science_stamp)\n",
    "                    ref = decode_stamp(reference_stamp)\n",
    "                    diff = decode_stamp(difference_stamp)\n",
    "\n",
    "                    if sci is not None and ref is not None and diff is not None:\n",
    "                        stamps_3d = np.stack([sci, ref, diff])\n",
    "\n",
    "                        # Recolecta metadatos relevantes\n",
    "                        metadata = {\n",
    "                            \"ra\": candidate.get(\"ra\", -999),\n",
    "                            \"dec\": candidate.get(\"dec\", -999),\n",
    "                            \"magpsf\": candidate.get(\"magpsf\", -999),\n",
    "                            \"sigmapsf\": candidate.get(\"sigmapsf\", -999),\n",
    "                            \"isdiffpos\": int(candidate.get(\"isdiffpos\", 'f') == 't'),\n",
    "                            \"diffmaglim\": candidate.get(\"diffmaglim\", -999),\n",
    "                            \"fwhm\": candidate.get(\"fwhm\", -999),\n",
    "                            \"sgscore1\": candidate.get(\"sgscore1\", -999),\n",
    "                            \"sgscore2\": candidate.get(\"sgscore2\", -999),\n",
    "                            \"sgscore3\": candidate.get(\"sgscore3\", -999),\n",
    "                            \"distpsnr1\": candidate.get(\"distpsnr1\", -999),\n",
    "                            \"distpsnr2\": candidate.get(\"distpsnr2\", -999),\n",
    "                            \"distpsnr3\": candidate.get(\"distpsnr3\", -999),\n",
    "                            \"classtar\": candidate.get(\"classtar\", -999),\n",
    "                            \"ndethist\": candidate.get(\"ndethist\", -1),\n",
    "                            \"ncovhist\": candidate.get(\"ncovhist\", -1),\n",
    "                            \"chinr\": candidate.get(\"chinr\", -999),\n",
    "                            \"sharpnr\": candidate.get(\"sharpnr\", -999),\n",
    "                            \"gal_lat\": alert.get(\"gal_lat\", -999),\n",
    "                            \"gal_lng\": alert.get(\"gal_lng\", -999),\n",
    "                            \"ecl_lat\": alert.get(\"ecl_lat\", -999),\n",
    "                            \"ecl_lng\": alert.get(\"ecl_lng\", -999),\n",
    "                            \"approx_nondet\": candidate.get(\"ncovhist\", -1) - candidate.get(\"ndethist\", -1),\n",
    "                        }\n",
    "\n",
    "                        # Finalmente guarda como dict\n",
    "                        data_dict = {\n",
    "                            \"object_id\": object_id,\n",
    "                            \"data\": stamps_3d,\n",
    "                            \"metadata\": metadata\n",
    "                        }\n",
    "\n",
    "                        np.save(os.path.join(output_dir, f\"alert_{object_id}_full.npy\"), data_dict)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en archivo {avro_file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/seba/Usach/bigdata/Proyecto/datos1\"  # Reemplaza con la ruta al directorio con archivos Avro\n",
    "    store_ztf_stamps_3d_with_metadata(input_directory, output_dir=\"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_1\", crop_to_21x21=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd32f3b",
   "metadata": {},
   "source": [
    "### ¿Qué contiene cada .npy?\n",
    "contiene una especie de diccionario en donde se tienen los objet_id que será llamado como \"ZTF_algo...\", la data que será el array de imagenes (3x21x21), y finalmente el metadato que seran todos los metadatos que se nombraron en la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167dec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Normaliza los datos de entrada a un rango de 0 a 1.\n",
    "    \"\"\"\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    return (data - data_min) / (data_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c113d56",
   "metadata": {},
   "source": [
    "# Datos etiquetados \n",
    "Como nos interesa crear una red neuronal con capas convolucionales y densas para clasificar eventos transitorios necesitamos de datos etiquetados, los cuales pudimos encontrar en  https://zenodo.org/records/4279623 para luego realizar un crossmatch con nuestros datos a partir del id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd17c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alerce_data = pd.read_csv(\"/home/seba/Usach/bigdata/Proyecto/etiquetas/ALeRCE_lc_classifier_outputs_ZTF_unlabeled_set_20200609.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb6bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Etiquetas en alerce_data['predicted_class']:\n",
      "predicted_class\n",
      "Periodic-Other    243374\n",
      "E                 198122\n",
      "LPV               161592\n",
      "YSO                85087\n",
      "RRL                58592\n",
      "QSO                43054\n",
      "DSCT               26672\n",
      "CEP                17307\n",
      "AGN                14342\n",
      "CV/Nova             7945\n",
      "Blazar              5085\n",
      "SNIa                3956\n",
      "SNIbc               1626\n",
      "SNII                 890\n",
      "SLSN                 727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Muestra las etiquetas únicas y su número total en alerce_data\n",
    "print(\"\\nEtiquetas en alerce_data['predicted_class']:\")\n",
    "print(alerce_data['predicted_class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee43134",
   "metadata": {},
   "source": [
    "#### Data sets\n",
    "Nos interesa obtener grandes volumenes de datos para el entrenamiento, por lo tanto realizamos un codigo que se encarga de sacar variaos set de datos y juntarlos en un puro dataframe, vale recalcar que los datos son los extraidos anteriormente en binario (formato .npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbb890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos procesados: 142014 de 147156  \r"
     ]
    }
   ],
   "source": [
    "# Buscamos todos los archivos con datos completos\n",
    "folders = [\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_2\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_3\",\n",
    "    \"/home/seba/Usach/bigdata/Proyecto/ztf_3d_stamps_4\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for folder in folders:\n",
    "    file_list = glob.glob(os.path.join(folder, \"alert_*_full.npy\"))\n",
    "    total_files = len(file_list)\n",
    "\n",
    "    for j, path in enumerate(file_list, 1):\n",
    "        try:\n",
    "            data = np.load(path, allow_pickle=True).item()\n",
    "            rows.append({\n",
    "                \"object_id\": data[\"object_id\"],\n",
    "                \"data\": data[\"data\"],\n",
    "                **data[\"metadata\"]  # Desempaquetamos el dict de metadatos como columnas\n",
    "            })\n",
    "            print(f'Archivos procesados: {j} de {total_files}  ', end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando {path}: {e}\")\n",
    "\n",
    "\n",
    "# Converción a DataFrame\n",
    "avro_df = pd.DataFrame(rows)\n",
    "avro_df = avro_df.drop_duplicates(subset='object_id', keep='first') # eliminamos duplicados con el mismo object_id\n",
    "\n",
    "print(avro_df.shape)\n",
    "print(avro_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fb618",
   "metadata": {},
   "source": [
    "# Cross Match\n",
    "Ahora que tenemos el set de datos toca realizar el crossmatch, esto es importante ya que la red neuronal debe de saber que clase de evento transitorio está analizando (en nuestro caso nos decantamos por QSO, YSO , AGN, VS, SN y otros ), ademas de que el set de datos etiquetados no tiene imagenes y por lo tanto es completamente necesario realizar el crossmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "VS       97866\n",
      "Other    24169\n",
      "YSO       6931\n",
      "QSO       2791\n",
      "AGN       1931\n",
      "SN         103\n",
      "Name: count, dtype: int64\n",
      "      object_id                                               data  \\\n",
      "0  ZTF19aamuloq  [[[269.03296, 272.72073, 276.7851, 271.75027, ...   \n",
      "1  ZTF18abmrhkl  [[[98.008545, 98.33455, 103.00464, 93.19916, 8...   \n",
      "2  ZTF19aaydqdp  [[[337.72638, 336.0949, 336.17847, 334.89038, ...   \n",
      "3  ZTF18aakghjo  [[[143.20807, 146.21886, 154.38998, 142.27063,...   \n",
      "4  ZTF18abbrojy  [[[374.30634, 382.51877, 375.64142, 376.90683,...   \n",
      "\n",
      "           ra        dec     magpsf  sigmapsf  isdiffpos  diffmaglim  \\\n",
      "0  175.181255 -14.765624  19.262691  0.163901          1   19.746798   \n",
      "1  328.547468  31.060491  19.858799  0.119543          0   20.517921   \n",
      "2  209.058338 -26.614814  17.936068  0.090117          1   19.776381   \n",
      "3  232.806765  70.447667  16.847271  0.050514          0   20.256754   \n",
      "4  286.952025 -26.940841  16.846992  0.069402          0   19.772224   \n",
      "\n",
      "       fwhm  sgscore1  ...  ndethist  ncovhist  chinr  sharpnr  gal_lat  \\\n",
      "0  1.529680  0.998750  ...       276       829  0.945   -0.018     -999   \n",
      "1  3.250000  0.032917  ...       815      2071  4.540    0.368     -999   \n",
      "2  1.526580  0.973762  ...       324       651  0.404   -0.026     -999   \n",
      "3  1.401479  0.997500  ...       866      2784  0.491   -0.019     -999   \n",
      "4  1.754663  0.979792  ...       296       806  0.570    0.028     -999   \n",
      "\n",
      "   gal_lng  ecl_lat  ecl_lng  approx_nondet  label  \n",
      "0     -999     -999     -999            553     VS  \n",
      "1     -999     -999     -999           1256    AGN  \n",
      "2     -999     -999     -999            327     VS  \n",
      "3     -999     -999     -999           1918     VS  \n",
      "4     -999     -999     -999            510     VS  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Definimos el mapeo de clases a las nuevas etiquetas\n",
    "label_map = {\n",
    "    'AGN': 'AGN',\n",
    "    'Blazar': 'AGN',\n",
    "    'QSO': 'QSO',\n",
    "    'YSO': 'YSO',\n",
    "    'E': 'VS',\n",
    "    'LPV': 'VS',\n",
    "    'RRL': 'VS',\n",
    "    'DSCT': 'VS',\n",
    "    'CEP': 'VS',\n",
    "    'SNIa': 'Other',\n",
    "    'SNIbc': 'Other',\n",
    "    'SNII': 'Other',\n",
    "    'SLSN': 'Other',\n",
    "    'Periodic-Other': 'Other',\n",
    "    'CV/Nova': 'Other'\n",
    "}\n",
    "\n",
    "# Asignamos la nueva etiqueta según el mapeo, si no está en el mapeo se pone 'Other'\n",
    "alerce_data['label'] = alerce_data['predicted_class'].map(label_map).fillna('Other')\n",
    "\n",
    "# Seleccionamos solo las columnas necesarias y renombramos para el merge\n",
    "alerce_labels = alerce_data[['oid', 'label']].rename(columns={'oid': 'object_id'})\n",
    "\n",
    "# Cross-match con avro_data\n",
    "final_avro_df = pd.merge(avro_df, alerce_labels, on='object_id', how='inner')\n",
    "\n",
    "print(final_avro_df['label'].value_counts())\n",
    "print(final_avro_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7416a2",
   "metadata": {},
   "source": [
    "#### Guardar Dataset en varios pickle\n",
    "En nuestro caso nos acomoda guardar los datos formato pickle, ya que la estructura de nuestros datos es compleja debido a la incorporacion de metadatos con sus respectiva imagenes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75985f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos por cada clase y tomamos una muestra eleatoria de 12000 ejemplos de la clase 'Other' y 'Variable_Star'para balancear el dataset\n",
    "\n",
    "Other = final_avro_df[final_avro_df['label'] == 'Other'].sample(n=12000, random_state=42)\n",
    "Other = Other.reset_index(drop=True)\n",
    "Other['data'] = Other['data'].apply(normalize_data)\n",
    "\n",
    "Quasar = final_avro_df[final_avro_df['label'] == 'QSO']\n",
    "Quasar = Quasar.reset_index(drop=True)\n",
    "Quasar['data'] = Quasar['data'].apply(normalize_data)\n",
    "\n",
    "AGN = final_avro_df[final_avro_df['label'] == 'AGN']\n",
    "AGN = AGN.reset_index(drop=True)\n",
    "AGN['data'] = AGN['data'].apply(normalize_data)\n",
    "\n",
    "YSO = final_avro_df[final_avro_df['label'] == 'YSO']\n",
    "YSO = YSO.reset_index(drop=True)\n",
    "YSO['data'] = YSO['data'].apply(normalize_data)\n",
    "\n",
    "Variable_Star = final_avro_df[final_avro_df['label'] == 'VS'].sample(n=12000, random_state=42)\n",
    "Variable_Star = Variable_Star.reset_index(drop=True)\n",
    "Variable_Star['data'] = Variable_Star['data'].apply(normalize_data)\n",
    "\n",
    "\n",
    "# Finalmente como nos indico nuestro especialista en redes neuronales, \n",
    "# guardaremos cada clase en un .pickle separado\n",
    "Quasar.to_pickle(\"ztf_avro_Grande_QSO.pkl\")\n",
    "AGN.to_pickle(\"ztf_avro_Grande_AGN.pkl\")\n",
    "YSO.to_pickle(\"ztf_avro_Grande_YSO.pkl\")\n",
    "Other.to_pickle(\"ztf_avro_Grande_Other.pkl\")\n",
    "Variable_Star.to_pickle(\"ztf_avro_Grande_Variable_Star.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16585244",
   "metadata": {},
   "source": [
    "# En resumen\n",
    "### el codigo lo que hace es:\n",
    "- Procesar los datos de alerta ZTF desde archivos avro\n",
    "- Extraer las imagenes en formato 3D (3 canales) para cada alerta (también se limpian los datos transformando los NaN a 0)\n",
    "- Extraer una amplia selección de metadata de las imagenes ñas cuales servirán para el entrenamiento de la red neuronal (también nos aseguramos que no hayan datos nan reemplazando por 0 o por -999 según el tipo de dato)\n",
    "- Unimos los datos con etiquetas entregadas por alerce (hacemos un crossmatch con un dataset pequeño el cual tiene datos etiquetados)\n",
    "- Filtramos por clases de nuestro interes y agregamos una nueva clase para evitar error de clasificación (QSO, YSO , AGN, VS y otros)\n",
    "- Exportamos el conjunto de datos etiquetados en pickle para futuros entrenamientos de la red neuronal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
